{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.telemetry import set_diagnostics_collection\n",
    "\n",
    "from azureml.core.workspace import Workspace\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name : ' + ws.name,\n",
    "      'Azure region   : ' + ws.location,\n",
    "      'Subscription id: ' + ws.subscription_id,\n",
    "      'Resource group : ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package scoring code to be sent to all scoring nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "# Create a directory that will contain all the necessary code\n",
    "# that will need to be accessed on the compute targets for execution of the steps\n",
    "source_directory = './scoring_runtime'\n",
    "if os.path.exists(source_directory):\n",
    "    shutil.rmtree(source_directory)\n",
    "os.makedirs(source_directory, exist_ok=True)\n",
    "shutil.copy('./pre_processing.py', source_directory) # the pre_processing code\n",
    "shutil.copy('./scoring.py', source_directory) # the scoring code\n",
    "shutil.copy('./scoring_custom_package.py', source_directory) # a custom package you may want to 'import' in your scoring.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring Pipeline Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment,Datastore\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.pipeline.core import Pipeline,PipelineData\n",
    "from azureml.pipeline.core.pipeline_output_dataset import PipelineOutputFileDataset\n",
    "from azureml.pipeline.steps import ParallelRunStep, ParallelRunConfig\n",
    "\n",
    "# Environments Definition\n",
    "cpu_env = Environment.from_conda_specification(name = \"ubuntu\",file_path = \"./conda-scoring-cpu.yml\")\n",
    "cpu_env.docker.base_image = \"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\"\n",
    "cpu_env.docker.enabled = True\n",
    "gpu_env = Environment.from_conda_specification(name = \"ubuntu-tf-gpu-1.15.3\",file_path = \"./conda-scoring-gpu.yml\")\n",
    "gpu_env.docker.base_image = \"mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04\"\n",
    "gpu_env.docker.enabled = True\n",
    "\n",
    "# Compute Targets\n",
    "cpu_compute_target = ComputeTarget(workspace=ws, name=\"f8s-cc\")\n",
    "gpu_compute_target = ComputeTarget(workspace=ws, name=\"nv12-cc\")\n",
    "print(f\"CPU Compute Target: {cpu_compute_target.name}\")\n",
    "print(f\"GPU Compute Target: {gpu_compute_target.name}\")\n",
    "\n",
    "# control parallelism for earch compute target type\n",
    "cpu_node_count=1\n",
    "cpu_process_count_per_node=8 # set this to number of vcores per compute node\n",
    "gpu_node_count=1\n",
    "gpu_process_count_per_node=2 # set this to number of gpus per compute node\n",
    "\n",
    "# scoring input\n",
    "images_to_score_ni = Dataset.get_by_name(ws, name='images-to-score').as_named_input('images_to_score')\n",
    "\n",
    "# intermediate data: the pre-processing step in this example will identify images with problems before scoring and set them aside\n",
    "data_store = Datastore.get_default(ws)\n",
    "images_pre_processed_pd = PipelineOutputFileDataset(PipelineData(name=\"images_pre_processed\",datastore=data_store))\n",
    "\n",
    "# scoring output\n",
    "images_scored_pd = PipelineData(name=\"images_scored\",datastore=data_store)\n",
    "\n",
    "# pre_processing step\n",
    "pre_processing_run_config = ParallelRunConfig(\n",
    "    source_directory=source_directory,\n",
    "    entry_script=\"pre_processing.py\",\n",
    "    mini_batch_size=\"4\",\n",
    "    error_threshold=1,\n",
    "    output_action=\"append_row\",\n",
    "    append_row_file_name=\"pre_processing.csv\",\n",
    "    environment=cpu_env,\n",
    "    compute_target=cpu_compute_target,\n",
    "    node_count=cpu_node_count,\n",
    "    process_count_per_node=cpu_process_count_per_node\n",
    ")\n",
    "pre_processing_step = ParallelRunStep(\n",
    "    name=\"pre-processing\",\n",
    "    parallel_run_config=pre_processing_run_config,\n",
    "    arguments=[\"--images-pre-processed-folder\", images_pre_processed_pd],\n",
    "    inputs=[images_to_score_ni],\n",
    "    output=images_pre_processed_pd,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "# scoring step\n",
    "model_name = \"tfod_model\"\n",
    "model_version = 11\n",
    "inference_batch_size = 4 # how many images are scored at a once on each GPU: try to max out the memory of the GPU for maximum speed\n",
    "scoring_run_config = ParallelRunConfig(\n",
    "    source_directory=source_directory,\n",
    "    entry_script=\"scoring.py\",\n",
    "    mini_batch_size=\"4\",\n",
    "    error_threshold=1,\n",
    "    output_action=\"append_row\",\n",
    "    append_row_file_name=\"scoring.csv\",\n",
    "    environment=gpu_env,\n",
    "    compute_target=gpu_compute_target,\n",
    "    node_count=gpu_node_count,\n",
    "    process_count_per_node=gpu_process_count_per_node\n",
    ")\n",
    "scoring_step = ParallelRunStep(\n",
    "    name=\"scoring\",\n",
    "    parallel_run_config=scoring_run_config,\n",
    "    arguments=[\"--images-scored-folder\", images_scored_pd,\n",
    "              \"--model-name\", model_name,\n",
    "              \"--model-version\", model_version,\n",
    "              \"--inference-batch-size\", inference_batch_size],\n",
    "    inputs=[images_pre_processed_pd],\n",
    "    output=images_scored_pd,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[scoring_step])\n",
    "print(f\"Pipeline: {pipeline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'tf-batch-scoring'\n",
    "experiment = Experiment(ws, name=experiment_name)\n",
    "run = experiment.submit(pipeline,tags={'cpu_nodes': str(cpu_node_count), 'gpu_nodes': str(gpu_node_count)})\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publish the Pipeline as an Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from azureml.pipeline.core import PipelineEndpoint\n",
    "\n",
    "pipeline_endpoint_name = \"tf-batch-scoring\"\n",
    "pipeline_endpoint = PipelineEndpoint.publish(workspace=ws,\n",
    "                                                name=pipeline_endpoint_name,\n",
    "                                                pipeline=pipeline,\n",
    "                                                description=\"Tensorflow Batch Scoring\")\n",
    "print(f\"Pipeline endpoint: {pipeline_endpoint}\")'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
